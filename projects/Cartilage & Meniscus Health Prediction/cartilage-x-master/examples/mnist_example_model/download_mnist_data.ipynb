{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Downloading and Formatting Data for MNIST\n",
    "Used to get the data for the convolutional neural network in the other notebook. Might be useful for testing how different model architectures function with a larger dataset. There's a LOT here we don't need but some functions here might be useful for our images.\n",
    "\n",
    "A lot of this is borrowed from Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import bunch of useful packages\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import imagehash  # needs to be installed\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions to download and extract datasets from a URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write('%s%%' % percent)\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            sys.stdout.write('.')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        last_percent_reported = percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "    dest_filename = os.path.join(data_root, filename)\n",
    "    if force or not os.path.exists(dest_filename):\n",
    "        print('Attempting to download', filename)\n",
    "        filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "        print('\\nDownload complete!')\n",
    "    statinfo = os.stat(dest_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', dest_filename)\n",
    "    else:\n",
    "        print('Failed to verify' + dest_filename + '. Can you get to it with a browser?')\n",
    "    return dest_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_extract(filename, force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0]\n",
    "    if os.path.isdir(root) and not force:\n",
    "        print('%s already present - skipping extraction of %s' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s - this may take a while.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(data_root)\n",
    "        tar.close()\n",
    "    data_folders = [\n",
    "        os.path.join(root, d) for d in sorted(os.listdir(root)) if os.path.isdir(os.path.join(root, d))\n",
    "    ]\n",
    "    if len(data_folders) != num_classes:\n",
    "        raise Exception(\"Expected %s folders, found %s instead\" % (num_classes, len(data_folders)))\n",
    "    return data_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to check data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_image_samples(data_folders):\n",
    "    for data_folder in data_folders:\n",
    "        sample_image_files = [os.path.join(data_folder, os.listdir(data_folder)[i]) for i in range(1)]\n",
    "        for image_file in sample_image_files:\n",
    "            if os.path.isfile(image_file):\n",
    "                image = plt.imread(image_file)\n",
    "                plt.title(data_folder)\n",
    "                plt.imshow(image)\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"Loads data for a single letter\"\"\"\n",
    "    image_size = 28\n",
    "    pixel_depth = 255.0\n",
    "    image_files = [os.path.join(folder, file) for file in os.listdir(folder)]\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "    num_images = 0\n",
    "    for image_file in image_files:\n",
    "        try:\n",
    "            image_data = (ndimage.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception(\"Unexpected image shape: %s\" % str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images += 1\n",
    "        except IOError as e:\n",
    "            print(\"Could not read image file: \", e, \". It\\'s ok - skipping\")\n",
    "\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    if num_images <= min_num_images:\n",
    "        raise Exception(\"Significantly fewer images than expected: %s < %s - please check.\"\n",
    "                        % (num_images, min_num_images))\n",
    "    print(\"Full dataset tensor: \", dataset.shape)\n",
    "    print(\"Mean:\\t%s\" % np.mean(dataset))\n",
    "    print(\"Stddev: %s\" % np.std(dataset))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangling, storing, and merging data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "        if os.path.exists(set_filename) and not force:\n",
    "            # You may override by setting force=True.\n",
    "            print('%s already present - Skipping pickling.' % set_filename)\n",
    "        else:\n",
    "            print('Pickling %s.' % set_filename)\n",
    "            dataset = load_letter(folder, min_num_images_per_class)\n",
    "            try:\n",
    "                with open(set_filename, 'wb') as f:\n",
    "                    pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "            except Exception as e:\n",
    "                print('Unable to save data to', set_filename, ':', e)\n",
    "\n",
    "    return dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_arrays(row_num, img_size):\n",
    "    if row_num:\n",
    "        dataset = np.ndarray((row_num, img_size, img_size), dtype=np.float32)\n",
    "        labels = np.ndarray(row_num, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combines all datasets -- this remove similars flag makes this more complicated than it would otherwise be\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0, remove_similar=False):\n",
    "    num_classes = len(pickle_files)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "\n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class + tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                # let's shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if remove_similar and label != 8:\n",
    "                    letter_set, _ = remove_similars(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter_set = letter_set[:vsize_per_class, :, :]\n",
    "                    valid_dataset[start_v:end_v, :, :] = valid_letter_set\n",
    "                    valid_labels[start_v:end_v] = label\n",
    "                    start_v += vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "\n",
    "                train_letter_set = letter_set[vsize_per_class:end_l, :, :]\n",
    "                train_dataset[start_t:end_t, :, :] = train_letter_set\n",
    "                train_labels[start_t:end_t] = label\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "\n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomise(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any duplicates need to be removed -- I will include another script that I used to remove images that are too similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_similars(dataset, labels=None):\n",
    "    num_before = len(dataset)\n",
    "    hashed_dataset = [hash_image(img) for img in dataset]\n",
    "    _, index_u = np.unique(hashed_dataset, return_index=True)\n",
    "    if labels:\n",
    "        labels = labels[index_u]\n",
    "    num_after = len(index_u)\n",
    "    dataset = dataset[index_u, :, :]\n",
    "    print(\"{} repeated samples removed of {}\".format(num_before - num_after, num_before))\n",
    "    return dataset, labels\n",
    "\n",
    "def hash_image(image):\n",
    "    image_int = (image * 255 + 255).astype(np.int32)\n",
    "    hashed_image = imagehash.dhash(Image.fromarray(image_int).resize((16, 16), Image.ANTIALIAS))\n",
    "    return str(hashed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_image_sample_pretty(dataset, labels):\n",
    "    alph = 'ABCDEFGHIJ'\n",
    "    indexes = np.random.permutation(len(labels))[0:12]\n",
    "    sample_images = dataset[indexes, :, :]\n",
    "    sample_labels = labels[indexes]\n",
    "\n",
    "    for i in range(len(indexes)):\n",
    "        alph_label = alph[sample_labels[i]]\n",
    "        plt.subplot(3, 4, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.title(alph_label)\n",
    "        plt.imshow(sample_images[i, :, :])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified mnist/notMNIST_large.tar.gz\n",
      "Found and verified mnist/notMNIST_small.tar.gz\n",
      "mnist/notMNIST_large already present - skipping extraction of mnist/notMNIST_large.tar.gz\n",
      "mnist/notMNIST_small already present - skipping extraction of mnist/notMNIST_small.tar.gz\n",
      "mnist/notMNIST_large\\A.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_large\\B.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_large\\C.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_large\\D.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_large\\E.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_large\\F.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_large\\G.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_large\\H.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_large\\I.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_large\\J.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_small\\A.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_small\\B.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_small\\C.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_small\\D.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_small\\E.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_small\\F.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_small\\G.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_small\\H.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_small\\I.pickle already present - Skipping pickling.\n",
      "mnist/notMNIST_small\\J.pickle already present - Skipping pickling.\n",
      "133 repeated samples removed of 1872\n",
      "177 repeated samples removed of 1873\n",
      "61 repeated samples removed of 1873\n",
      "219 repeated samples removed of 1873\n",
      "69 repeated samples removed of 1873\n",
      "77 repeated samples removed of 1872\n",
      "53 repeated samples removed of 1872\n",
      "612 repeated samples removed of 1872\n",
      "72 repeated samples removed of 1872\n",
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n",
      "Compressed size:  690800503\n"
     ]
    }
   ],
   "source": [
    "# Setting URL and folder for data\n",
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = 'mnist/'  # make sure you create this folder manually before running this cell\n",
    "\n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n",
    "\n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)\n",
    "\n",
    "# pprint(train_folders)\n",
    "# display_image_samples(train_folders)\n",
    "# display_image_samples(test_folders)\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)\n",
    "\n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "    train_datasets, train_size, valid_size, remove_similar=False)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size, remove_similar=True)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "train_dataset, train_labels = randomise(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = randomise(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = randomise(test_dataset, test_labels)\n",
    "\n",
    "pickle_file = os.path.join(data_root, 'MNISTu.pickle')\n",
    "\n",
    "try:\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        save = {\n",
    "            'train_dataset': train_dataset,\n",
    "            'train_labels': train_labels,\n",
    "            'valid_dataset': valid_dataset,\n",
    "            'valid_labels': valid_labels,\n",
    "            'test_dataset': test_dataset,\n",
    "            'test_labels': test_labels\n",
    "        }\n",
    "        pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "except Exception as e:\n",
    "    print(\"Unable to save pickle file: \", pickle_file, \": \", e)\n",
    "    raise\n",
    "\n",
    "statinfo = os.stat(pickle_file)\n",
    "print(\"Compressed size: \", statinfo.st_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this make sure you don't push the downloaded MNIST data to the repository -- it'll take really long to clone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
