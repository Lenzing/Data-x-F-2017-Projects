{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Convolutional Neural Network (CNN) - MNIST Example\n",
    "This is a script using tensorflow and the MNIST handwritten character dataset to build a multiple-class classifier -- the script to download the data onto your own. \n",
    "\n",
    "I'll draft up a (neater and better) one for a 3D image soon. This dataset might be useful for testing models on larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TensorFlow\n",
    "To get tensorflow to work I had to create a new anaconda environment with Python 3.5:\n",
    "```bash\n",
    "> conda create -n data-x-tf python=3.5 anaconda\n",
    "```\n",
    "\n",
    "To install tensorflow, run the following commands:\n",
    "```bash\n",
    "> activate data-x-tf\n",
    "> pip install --ignore-installed --upgrade tensorflow\n",
    "```\n",
    "Let me know if you have any issues with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from six.moves import cPickle as pickle  # useful package for saving and loading all sorts of datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locate saved pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_root = 'mnist/'\n",
    "pickle_file = os.path.join(data_root, 'MNISTu.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From file:\n",
      "Train:  (200000, 28, 28) (200000,)\n",
      "Valid:  (10000, 28, 28) (10000,)\n",
      "Test:  (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load data from pickle file\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save\n",
    "\n",
    "print(\"From file:\")\n",
    "print(\"Train: \", train_dataset.shape, train_labels.shape)\n",
    "print(\"Valid: \", valid_dataset.shape, valid_labels.shape)\n",
    "print(\"Test: \", test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Setting everything up\n",
    "Information about the model and some convenience functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_labels = 10\n",
    "image_size = 28\n",
    "num_channels = 1\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return 100.0 * np.sum((np.argmax(predictions, 1) == np.argmax(labels, 1))) / predictions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-checking the data that's been loaded in from the pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped\n",
      "Train:  (200000, 28, 28, 1) (200000, 10)\n",
      "Valid:  (10000, 28, 28, 1) (10000, 10)\n",
      "Test:  (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print(\"Reshaped\")\n",
    "print(\"Train: \", train_dataset.shape, train_labels.shape)\n",
    "print(\"Valid: \", valid_dataset.shape, valid_labels.shape)\n",
    "print(\"Test: \", test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining more model parameters -- hyperparameters and CNN architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining parameters for the model architecture\n",
    "batch_size = 32\n",
    "patch_size = 5\n",
    "k_size = 2\n",
    "depth1 = 32\n",
    "depth2 = 16\n",
    "depth3 = 8\n",
    "num_h1 = 32\n",
    "num_h2 = 16\n",
    "\n",
    "# hyperparameters\n",
    "train_subset = 200000  # amount of the full dataset to use\n",
    "lbd = 0.0000  # amount of regularization\n",
    "keep_prob = 0.5  # this is for something called dropout -- another type of regularization\n",
    "learning_rate = 0.1  # like 'alpha' in linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the computation graph -- the 'tensors'\n",
    "This sets up the computation network/nodes without actually calculating or running anything. This model (by no means the best option) includes the following:\n",
    "- 28x28x1 input images\n",
    "- stochastic optimization - training batchs of 32 images\n",
    "- 2x2x32 2d conv + relu (rectified linear unit -- just adds a cheap non-linearity)\n",
    "- 2x2 max pool\n",
    "- 2x2x16 2d conv + relu\n",
    "- 2x2 max pool\n",
    "- 2x2x8 2d conv + relu\n",
    "- 32 fully connected layers + relu\n",
    "- 16 fully connected layers\n",
    "- 10 output classes\n",
    "\n",
    "It's a bit of a mess here but I can rewrite these parts to be more general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.device('/gpu:0'):\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "        tf_valid_dataset = tf.constant(valid_dataset)\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "        w1 = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth1], stddev=0.1))\n",
    "        b1 = tf.Variable(tf.zeros([depth1]))\n",
    "\n",
    "        w2 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth1, depth2], stddev=0.1))\n",
    "        b2 = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "\n",
    "        w3 = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth2, depth3], stddev=0.1))\n",
    "        b3 = tf.Variable(tf.constant(1.0, shape=[depth3]))\n",
    "\n",
    "        w4 = tf.Variable(tf.truncated_normal([depth3 * 4 * 4, num_h1],\n",
    "                                             stddev=0.1))\n",
    "        b4 = tf.Variable(tf.constant(1.0, shape=[num_h1]))\n",
    "\n",
    "        w5 = tf.Variable(tf.truncated_normal([num_h1, num_h2], stddev=0.1))\n",
    "        b5 = tf.Variable(tf.constant(1.0, shape=[num_h2]))\n",
    "\n",
    "        w6 = tf.Variable(tf.truncated_normal([num_h2, num_labels], stddev=0.1))\n",
    "        b6 = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "        def model(dataset):\n",
    "            conv = tf.nn.conv2d(dataset, w1, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + b1)\n",
    "            hidden = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "            conv = tf.nn.conv2d(hidden, w2,  [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + b2)\n",
    "            hidden = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "            conv = tf.nn.conv2d(hidden, w3,  [1, 1, 1, 1], padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + b3)\n",
    "            hidden = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "            shape = hidden.get_shape().as_list()\n",
    "            reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, w4) + b4)\n",
    "            hidden = tf.nn.relu(tf.matmul(hidden, w5) + b5)\n",
    "            return tf.matmul(hidden, w6) + b6\n",
    "\n",
    "\n",
    "    def model_drop(dataset):\n",
    "        h1 = tf.matmul(dataset, w1) + b1\n",
    "        h1 = tf.nn.dropout(tf.nn.relu(h1), keep_prob)\n",
    "        h2 = tf.matmul(h1, w2) + b2\n",
    "        h2 = tf.nn.dropout(tf.nn.relu(h2), keep_prob)\n",
    "        h3 = tf.matmul(h2, w3) + b3\n",
    "        h3 = tf.nn.dropout(tf.nn.relu(h3), keep_prob)\n",
    "        return tf.matmul(h3, w4) + b4\n",
    "\n",
    "\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) \\\n",
    "        + lbd * (tf.nn.l2_loss(w1)\n",
    "                 + tf.nn.l2_loss(w2)\n",
    "                 + tf.nn.l2_loss(w3)\n",
    "                 + tf.nn.l2_loss(w4)\n",
    "                 + tf.nn.l2_loss(w5))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    a_optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    train_prediction = tf.nn.softmax(model(tf_train_dataset))\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the graph in a 'session'\n",
    "A session is where data is actually fed into the model and it is trained. Or alternatively, an already-trained model is used to predict something.\n",
    "\n",
    "Each time `session.run()` is called, it runs one 'iteration' of the graph we defined above -- in this case, one step of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Loss at step 0: 2.333632\n",
      "Batch accuracy: 9.4%\n",
      "Valid accuracy: 10.0%\n",
      "\n",
      "Loss at step 1000: 0.354879\n",
      "Batch accuracy: 84.4%\n",
      "Valid accuracy: 85.5%\n",
      "\n",
      "Loss at step 2000: 0.642489\n",
      "Batch accuracy: 90.6%\n",
      "Valid accuracy: 87.5%\n",
      "\n",
      "Loss at step 3000: 0.550981\n",
      "Batch accuracy: 81.2%\n",
      "Valid accuracy: 88.3%\n",
      "\n",
      "Loss at step 4000: 0.591909\n",
      "Batch accuracy: 81.2%\n",
      "Valid accuracy: 88.9%\n",
      "\n",
      "Loss at step 5000: 0.188805\n",
      "Batch accuracy: 96.9%\n",
      "Valid accuracy: 89.5%\n",
      "\n",
      "Loss at step 6000: 0.377794\n",
      "Batch accuracy: 84.4%\n",
      "Valid accuracy: 89.8%\n",
      "\n",
      "Loss at step 7000: 0.209640\n",
      "Batch accuracy: 90.6%\n",
      "Valid accuracy: 89.9%\n",
      "\n",
      "Loss at step 8000: 0.413695\n",
      "Batch accuracy: 87.5%\n",
      "Valid accuracy: 90.0%\n",
      "\n",
      "Loss at step 9000: 0.438875\n",
      "Batch accuracy: 87.5%\n",
      "Valid accuracy: 89.9%\n",
      "\n",
      "Loss at step 10000: 0.319083\n",
      "Batch accuracy: 93.8%\n",
      "Valid accuracy: 90.2%\n",
      "\n",
      "Loss at step 11000: 0.320049\n",
      "Batch accuracy: 90.6%\n",
      "Valid accuracy: 90.3%\n",
      "\n",
      "Loss at step 12000: 0.391540\n",
      "Batch accuracy: 84.4%\n",
      "Valid accuracy: 90.6%\n",
      "\n",
      "Loss at step 13000: 0.183492\n",
      "Batch accuracy: 96.9%\n",
      "Valid accuracy: 90.5%\n",
      "\n",
      "Loss at step 14000: 0.152999\n",
      "Batch accuracy: 96.9%\n",
      "Valid accuracy: 90.5%\n",
      "\n",
      "Loss at step 15000: 0.404882\n",
      "Batch accuracy: 84.4%\n",
      "Valid accuracy: 90.6%\n",
      "\n",
      "Loss at step 16000: 0.342521\n",
      "Batch accuracy: 90.6%\n",
      "Valid accuracy: 90.6%\n",
      "\n",
      "Loss at step 17000: 0.383303\n",
      "Batch accuracy: 90.6%\n",
      "Valid accuracy: 90.8%\n",
      "\n",
      "Loss at step 18000: 0.253079\n",
      "Batch accuracy: 93.8%\n",
      "Valid accuracy: 90.7%\n",
      "\n",
      "Loss at step 19000: 0.077510\n",
      "Batch accuracy: 96.9%\n",
      "Valid accuracy: 90.8%\n",
      "\n",
      "Loss at step 20000: 0.333043\n",
      "Batch accuracy: 90.6%\n",
      "Valid accuracy: 90.5%\n",
      "\n",
      "Loss at step 21000: 0.305163\n",
      "Batch accuracy: 84.4%\n",
      "Valid accuracy: 90.8%\n",
      "\n",
      "Loss at step 22000: 0.068094\n",
      "Batch accuracy: 100.0%\n",
      "Valid accuracy: 90.9%\n",
      "\n",
      "Loss at step 23000: 0.161260\n",
      "Batch accuracy: 96.9%\n",
      "Valid accuracy: 90.9%\n",
      "\n",
      "Loss at step 24000: 0.262590\n",
      "Batch accuracy: 90.6%\n",
      "Valid accuracy: 90.6%\n",
      "\n",
      "Loss at step 25000: 0.344870\n",
      "Batch accuracy: 84.4%\n",
      "Valid accuracy: 91.0%\n",
      "Test accuracy: 96.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 25001\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.65\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = False\n",
    "\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}\n",
    "\n",
    "        _, l, predictions = session.run([a_optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(\"\\nLoss at step %i: %f\" % (step, l))\n",
    "            print(\"Batch accuracy: %.1f%%\" % (accuracy(predictions, train_labels[offset:(offset + batch_size), :])))\n",
    "            print(\"Valid accuracy: %.1f%%\" % (accuracy(valid_prediction.eval(), valid_labels)))\n",
    "\n",
    "    print(\"Test accuracy: %.1f%%\" % (accuracy(test_prediction.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
