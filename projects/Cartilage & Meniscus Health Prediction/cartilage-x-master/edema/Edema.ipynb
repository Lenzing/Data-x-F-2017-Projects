{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremy/anaconda/envs/venv/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import psutil\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "\n",
    "def get_indicies(matrix):\n",
    "    z_len = np.shape(matrix)[2]\n",
    "    indicies = []\n",
    "    for i in range(z_len):\n",
    "        slice = matrix[:,:,max(0, i - 1):i]\n",
    "        slice = np.sum(slice)\n",
    "        if slice != 0:\n",
    "            indicies.append(i)\n",
    "    return (min(indicies), max(indicies))\n",
    "\n",
    "def get_crop_size(matrix):\n",
    "    x = np.shape(matrix)[1]\n",
    "    y = np.shape(matrix)[0]\n",
    "    length_before_edge = 0\n",
    "    edges = []\n",
    "    \n",
    "    for i in range(x):\n",
    "        for j in range(y):\n",
    "            if matrix[i][j] != 0:\n",
    "                edges.append(length_before_edge)\n",
    "                length_before_edge = 0\n",
    "                break\n",
    "            length_before_edge += 1\n",
    "        length_before_edge = 0\n",
    "    return max(edges), min(edges)\n",
    "\n",
    "def save_1d_matrix(name, matrix, z_min, z_max, func=np.mean):\n",
    "    data_dir = 'dat/'\n",
    "    matrix = matrix[:,:,:z_min:z_max]\n",
    "    matrix = func(matrix, axis=2)\n",
    "    with open(data_dir + name, 'wb+') as f:\n",
    "        pickle.dump(matrix, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def merge_datasets(pickle_files, train_size, valid_size=0, remove_similar=False):\n",
    "    num_classes = len(pickle_files)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "\n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class + tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                # let's shuffle the letters to have random validation and training set\n",
    "                np.random.shuffle(letter_set)\n",
    "                if remove_similar and label != 8:\n",
    "                    letter_set, _ = remove_similars(letter_set)\n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter_set = letter_set[:vsize_per_class, :, :]\n",
    "                    valid_dataset[start_v:end_v, :, :] = valid_letter_set\n",
    "                    valid_labels[start_v:end_v] = label\n",
    "                    start_v += vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "\n",
    "                train_letter_set = letter_set[vsize_per_class:end_l, :, :]\n",
    "                train_dataset[start_t:end_t, :, :] = train_letter_set\n",
    "                train_labels[start_t:end_t] = label\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "\n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing all the data in batches\n",
    "# Make sure all the files are .mat. Slice if necessary\n",
    "\n",
    "mris = [f for f in os.listdir('../../mri/')]\n",
    "segs = [f for f in os.listdir('../../seg/')][1:]\n",
    "\n",
    "mri={}\n",
    "seg={}\n",
    "file_len = len(mris)\n",
    "batch_amount = 50\n",
    "end = 50\n",
    "\n",
    "# TODO: Determine batch amount base off of available memory psutil.virtual_memory()\n",
    "\n",
    "for i in range(0, file_len, batch_amount):\n",
    "    # Loads seg knee mris\n",
    "    if i > file_len:\n",
    "        break\n",
    "    if end > file_len:\n",
    "        end = file_len\n",
    "    for j in range(i, end):\n",
    "        mri[mris[j]] = scipy.io.loadmat('../../mri/' + mris[j])['im_store']\n",
    "        seg[segs[j]] = scipy.io.loadmat('../../seg/' + segs[j])['pred_con_vol']\n",
    "    \n",
    "    # Saves flattened knee mri as a pickle file\n",
    "    for segmentation in segs:\n",
    "        filename = segmentation.strip('.mat')\n",
    "        z_min, z_max = get_indicies(seg[segmentation])\n",
    "        save_1d_matrix(filename, mri[segmentation], z_min, z_max)\n",
    "        \n",
    "    end += batch_amount\n",
    "\n",
    "# Frees up memory\n",
    "mri = None; seg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "\n",
    "pickles = [f for f in os.listdir('dat/')]\n",
    "data = {}\n",
    "\n",
    "for filename in pickles:\n",
    "    with open('dat/' + filename, 'rb') as p:\n",
    "        x = pickle.load(p).ravel()\n",
    "        data[filename] = x\n",
    "        \n",
    "df = pd.DataFrame(data).T\n",
    "\n",
    "# TODO: Match each mri to the right values\n",
    "# df['hasEdema'] = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "\n",
    "train_size = 0\n",
    "valid_size = 0\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "    pickles, train_size, valid_size, remove_similar=False)\n",
    "\n",
    "#_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size, remove_similar=True)\n",
    "\n",
    "edema_svm = SVC()\n",
    "\n",
    "print ('Training a svm model..')\n",
    "edema_svm.fit(x_train, y_train)\n",
    "\n",
    "#Training Accuracy\n",
    "training_accuracy=edema_svm.score(x_train,y_train)\n",
    "print ('Training Accuracy:',training_accuracy)\n",
    "\n",
    "\n",
    "validation_accuracy=edema_svm.score(x_test,y_test)\n",
    "print('Accuracy of the model on unseen validation data: ',validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DNN\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
