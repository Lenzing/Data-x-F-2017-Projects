{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Match N Merge Notebook\n",
    "#This notebook depends on flexx (to install - conda install flexx)\n",
    "#To run the Match N Merge solution, \"Run All\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flexx.webruntime import launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO flexx.webruntime: launching BrowserRuntime\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<flexx.webruntime.browser.BrowserRuntime at 0x571f470>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this launches the UI,must launch in firefox\n",
    "#set default download file path to the current notebook folder\n",
    "fileToLaunch = \"file:///\" + os.getcwd() + \"/matchnmerge.html\"\n",
    "launch(fileToLaunch, 'firefox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attemped processing of file filesReady.txt\n",
      "attemped processing of file  status.txt\n",
      "attemped processing of file  columnMapping.csv\n",
      "attemped processing of file  results.csv\n",
      "attemped processing of file  results.txt\n"
     ]
    }
   ],
   "source": [
    "#clean up\n",
    "#clear all files in variable removeFiles\n",
    "#create status.txt\n",
    "import os\n",
    "removeFiles = \"filesReady.txt, status.txt, columnMapping.csv, results.csv, results.txt\"\n",
    "filesToRemove = removeFiles.split(\",\")\n",
    "for file in filesToRemove:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "        print(\"removed \" + file)\n",
    "    except OSError:\n",
    "        print(\"attemped processing of file \" + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create status.txt\n",
    "with open('status.txt','w') as f:\n",
    "    f.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to prepend line to file\n",
    "def line_prepender(filename, line):\n",
    "    import time\n",
    "    tm = time.strftime('%H:%M:%S')\n",
    "    with open(filename, 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write(line.rstrip('\\r\\n') + '\\n' + tm + \" : \" + content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to write to status.txt\n",
    "#write to status whatever you want\n",
    "def writeToStatus(line):\n",
    "    line_prepender(\"status.txt\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "tm = time.strftime('%a, %d %b %Y %H:%M:%S')\n",
    "line_prepender(\"status.txt\", tm + \":Starting execution ...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR flexx.webruntime: runtime process stopped (1), stdout:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wait for a file called, \"filesReady.txt\".  This is the file that is generated when the user has selected the two datasets\n",
    "# This file will contain two entries, separated by a comma.  They correspond to the dataset files (data set 1 and 2).\n",
    "# The files will be located at the same location as this notebook\n",
    "# This code waits for the existence of this file before proceeding\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "file_path = \"filesReady.txt\"\n",
    "while not os.path.exists(file_path):\n",
    "    time.sleep(5);\n",
    "    if os.path.isfile(file_path):\n",
    "        continue;\n",
    "    else:\n",
    "        time.sleep(1);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def columnJoiner():\n",
    "    \n",
    "    writeToStatus(\"importing packages\")\n",
    "    \n",
    "    # no warnings\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    # the usuals\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv('filesReady.txt', header=None)\n",
    "    \n",
    "    # visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # machine learning\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC, LinearSVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB # Gaussian Naive Bays\n",
    "    from sklearn.linear_model import Perceptron\n",
    "    from sklearn.linear_model import SGDClassifier #stochastic gradient descent\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    import xgboost as xgb\n",
    "\n",
    "    # evaluation\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    writeToStatus(\"importing datasets & transforming dataframes\")\n",
    "\n",
    "    # read dataset 1 - we will train with this dataset\n",
    "    file1 = str(df.iloc[0,0])\n",
    "    df1a = pd.read_csv(file1, encoding='latin-1')\n",
    "    df1a.head()\n",
    "\n",
    "    # flatten values into one column & create df with column name = key1\n",
    "    df1b = pd.DataFrame({'key1': df1a.values.flatten()})\n",
    "    df1b.head()\n",
    "\n",
    "    # list of column names from file1\n",
    "    list1 = list(df1a)\n",
    "\n",
    "    # matrix of column names cycled through the data\n",
    "    matrix1 = np.transpose(np.tile(np.eye(len(list1), dtype= int), len(df1b.index)//len(list1)))\n",
    "    dfmatrix1 = pd.DataFrame(matrix1, columns=[list1])\n",
    "\n",
    "    # matrix merged with flattened file\n",
    "    df1c = pd.merge(df1b, dfmatrix1, left_index=True, right_index=True, how='right')\n",
    "    df1c.head()\n",
    "\n",
    "    # drop NaN in key1 column, reset index\n",
    "    df1d = df1c.dropna(subset = ['key1']).reset_index(drop=True)\n",
    "    df1d.head()\n",
    "\n",
    "    # read dataset 2 - we will validate with this dataset\n",
    "    file2 = str(df.iloc[0,1])\n",
    "    df2a = pd.read_csv(file2, encoding='latin-1')\n",
    "    df2a.tail()\n",
    "\n",
    "    # flatten values into one column & create df with column name = key2\n",
    "    df2b = pd.DataFrame({'key2': df2a.values.flatten()})\n",
    "    df2b.head()\n",
    "\n",
    "    # list of column names from file2\n",
    "    list2 = list(df2a)\n",
    "\n",
    "    # matrix of column names cycled through the data\n",
    "    matrix2 = np.transpose(np.tile(np.eye(len(list2), dtype= int), len(df2b.index)//len(list2)))\n",
    "    dfmatrix2 = pd.DataFrame(matrix2, columns=[list2])\n",
    "\n",
    "    # matrix merged with flattened data\n",
    "    df2c = pd.merge(df2b, dfmatrix2, left_index=True, right_index=True, how='right')\n",
    "\n",
    "    # drop NaN in key2 column, reset index\n",
    "    df2d = df2c.dropna(subset = ['key2']).reset_index(drop=True)\n",
    "    df2d.head()\n",
    "\n",
    "    # next part is feature engineering\n",
    "\n",
    "    writeToStatus(\"commence feature engineering\")\n",
    "\n",
    "    from collections import Counter\n",
    "    import string\n",
    "\n",
    "    def count_letters(word, valid_letters=string.ascii_letters):\n",
    "        count = Counter(word)\n",
    "        return sum(count[letter] for letter in valid_letters)\n",
    "\n",
    "    def count_digits(word, valid_letters=string.digits):\n",
    "        count = Counter(word)\n",
    "        return sum(count[digits] for digits in valid_letters)\n",
    "\n",
    "    def count_whitespace(word, valid_letters=string.whitespace):\n",
    "        count = Counter(word)\n",
    "        return sum(count[whitespace] for whitespace in valid_letters)\n",
    "\n",
    "    def count_punctuation(word, valid_letters=string.punctuation):\n",
    "        count = Counter(word)\n",
    "        return sum(count[punctuation] for punctuation in valid_letters)\n",
    "\n",
    "    df1d['key1'] = df1d['key1'].astype('str')\n",
    "    df1d['len'] = df1d['key1'].apply(lambda x: len(x))\n",
    "    df1d['let'] = df1d['key1'].apply(count_letters)\n",
    "    df1d['num'] = df1d['key1'].apply(count_digits)\n",
    "    df1d['ws'] = df1d['key1'].apply(count_whitespace)\n",
    "    df1d['punc'] = df1d['key1'].apply(count_punctuation)\n",
    "    df1d['%let'] = df1d.let/df1d.len\n",
    "    df1d['%num'] = df1d.num/df1d.len\n",
    "    df1d['%ws'] = df1d.ws/df1d.len\n",
    "    df1d['%punc'] = df1d.punc/df1d.len\n",
    "    \n",
    "    df2d['key2'] = df2d['key2'].astype('str')\n",
    "    df2d['len'] = df2d['key2'].apply(lambda x: len(x))\n",
    "    df2d['let'] = df2d['key2'].apply(count_letters)\n",
    "    df2d['num'] = df2d['key2'].apply(count_digits)\n",
    "    df2d['ws'] = df2d['key2'].apply(count_whitespace)\n",
    "    df2d['punc'] = df2d['key2'].apply(count_punctuation)\n",
    "    df2d['%let'] = df2d.let/df2d.len\n",
    "    df2d['%num'] = df2d.num/df2d.len\n",
    "    df2d['%ws'] = df2d.ws/df2d.len\n",
    "    df2d['%punc'] = df2d.punc/df2d.len\n",
    "\n",
    "    # for now, drop the last nine columns (len:%punc) in dataset 1\n",
    "    df1matrix = df1d.iloc[:,:-9].drop('key1',axis=1)\n",
    "\n",
    "    # loop df1matrix and fill in column names for 1s\n",
    "    for y in df1matrix:\n",
    "        df1matrix[y] = [y if ele  == 1 \n",
    "        else 0 for ele in df1matrix[y]]\n",
    "\n",
    "    writeToStatus(\"create X's & Y's\")\n",
    "        \n",
    "    # create Y1\n",
    "    Y1 = pd.DataFrame({'Y1': df1matrix.replace(0, np.nan).bfill(1).iloc[:, 0]})\n",
    "    Y1 = Y1.reset_index(drop=True)\n",
    "    Y1.head()\n",
    "\n",
    "    # create X1\n",
    "    X1 = df1d.iloc[:,-9:]\n",
    "    X1.head()\n",
    "\n",
    "    # for now, drop the last nine columns (len:%punc) in dataset 2\n",
    "    df2matrix = df2d.iloc[:,:-9].drop('key2',axis=1)\n",
    "\n",
    "    # loop df2matrix and fill in column names for 1s\n",
    "    for y in df2matrix:\n",
    "        df2matrix[y] = [y if ele  == 1 \n",
    "        else 0 for ele in df2matrix[y]]\n",
    "\n",
    "    # create Y2\n",
    "    Y2 = pd.DataFrame({'Y2': df2matrix.replace(0, np.nan).bfill(1).iloc[:, 0]})\n",
    "    Y2 = Y2.reset_index(drop=True)\n",
    "    Y2.head()\n",
    "\n",
    "    # create X2\n",
    "    X2 = df2d.iloc[:,-9:]\n",
    "    X2.head()\n",
    "\n",
    "    # next is the training & validation\n",
    "\n",
    "    writeToStatus(\"creating model based on dataset1\")\n",
    "\n",
    "    # create model, use SVC as its traiing accuracy is fairly high\n",
    "    model = SVC(probability=True)\n",
    "    model.fit(X1, Y1)\n",
    "\n",
    "    # make predictions on X2 based on X1/Y1 model\n",
    "    Y2_pred = pd.DataFrame(data = model.predict(X2))\n",
    "    Y2_pred.head()\n",
    "\n",
    "    writeToStatus(\"making predictions on dataset2\")\n",
    "\n",
    "    # show max accuracies per row per column\n",
    "    dfmax = pd.Series(pd.DataFrame(model.predict_proba(X2)).max(axis=1))\n",
    "    dfmax.head()\n",
    "\n",
    "    # place Y2 predictions and Y2 actuals side by side\n",
    "    Y2['Y2_pred'] = Y2_pred\n",
    "    df = Y2.rename(index=str, columns={\"Y2\": \"Y2_actual\", \"Y2_pred\": \"Y2_pred\"})\n",
    "    df.head()\n",
    "\n",
    "    # show the count of each pair of Y2 prediction and Y2 actual\n",
    "    df = pd.crosstab(df.Y2_actual,df.Y2_pred).replace(0,np.nan).\\\n",
    "         stack().reset_index().rename(columns={0:'Count'})\n",
    "\n",
    "    # shows the total count of each Y2 actual category\n",
    "    df = df.join(df.groupby('Y2_actual')[\"Count\"].sum(), on = 'Y2_actual', rsuffix = 'All')\n",
    "\n",
    "    # calculate relative accruacy based on total predicted per category/total count per category\n",
    "    dfAcc = df[['Count']].div(df.CountAll, axis=0)\n",
    "    df['Relative_Match_Accuracy'] = dfAcc\n",
    "\n",
    "    # drop Count and CountAll columns\n",
    "    df = df.loc[df.reset_index().groupby(['Y2_actual'])['Relative_Match_Accuracy'].idxmax()].drop(['Count', 'CountAll'], axis=1)\n",
    "\n",
    "    # sort column order\n",
    "    df = df[['Y2_actual', 'Y2_pred', 'Relative_Match_Accuracy']]\n",
    "    \n",
    "    # rename columns\n",
    "    df.columns = ['Base_Column', 'Matched_Column', 'Relative_Match_Accuracy']\n",
    "\n",
    "    writeToStatus(\"Match N Merge For Columns Completed\")\n",
    "    \n",
    "    # write to columnMapping.csv\n",
    "    df.to_csv('columnMapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rlim\\AppData\\Local\\Continuum\\Anaconda3\\envs\\data-x\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "columnJoiner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO:   wait for a file called \"columnMapping_user.csv\".\n",
    "# This the file that is generated by the UI when the user reviews and acknowledges column mappings\n",
    "\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "file_path = \"columnMapping_user.csv\"\n",
    "while not os.path.exists(file_path):\n",
    "    time.sleep(5);\n",
    "    if os.path.isfile(file_path):\n",
    "        continue;\n",
    "    else:\n",
    "        time.sleep(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is Rob's code\n",
    "def rowJoiner2():\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import difflib as dl\n",
    "    import math\n",
    "    from functools import partial\n",
    "    import os\n",
    "    import string\n",
    "    \n",
    "    def str_diff(tarString, testString) :\n",
    "        seq = dl.SequenceMatcher(None,tarString, str(testString))\n",
    "        ratioNum = seq.ratio()\n",
    "        return ratioNum\n",
    "    \n",
    "    def most_likely(testDF, tarString):\n",
    "        tempDF = pd.DataFrame()\n",
    "        tempDF['distance'] = np.NAN\n",
    "        tempDF['distance'] = testDF.apply(partial(str_diff,tarString))\n",
    "        maxValue = tempDF['distance'].max()\n",
    "        maxIndex = tempDF['distance'].idxmax()\n",
    "        #print('maxValue', maxValue)\n",
    "        int(maxIndex)\n",
    "        #print('maxIndex', maxIndex)\n",
    "        return maxIndex\n",
    "    \n",
    "    def most_likelyValue(testDF, tarString):\n",
    "        tempDF = pd.DataFrame()\n",
    "        tempDF['distance'] = np.NAN\n",
    "        tempDF['distance'] = testDF.apply(partial(str_diff,tarString))\n",
    "        maxValue = tempDF['distance'].max()\n",
    "        maxIndex = tempDF['distance'].idxmax()\n",
    "        #print('maxValue', maxValue)\n",
    "        int(maxIndex)\n",
    "        #print('maxIndex', maxIndex)\n",
    "        return maxValue\n",
    "    \n",
    "    writeToStatus(\"Starting row merge\")\n",
    "    \n",
    "    df = pd.read_csv('filesReady.txt', header=None)\n",
    "    \n",
    "    file1 = str(df.iloc[0,0])\n",
    "    firstDF = pd.read_csv(file1, encoding='latin-1', index_col=None)\n",
    "    \n",
    "    file2 = str(df.iloc[0,1])\n",
    "    secondDF = pd.read_csv(file2, encoding='latin-1', index_col=None)\n",
    "    \n",
    "   \n",
    "    topXRows = 1000\n",
    "        \n",
    "    mappedColumns = pd.read_csv(\"columnMapping_user.csv\", encoding='latin-1')\n",
    "    \n",
    "    #print('TOTAL DF\\n',mappedColumns)\n",
    "    \n",
    "    mappedColumns.loc[mappedColumns.iloc[:,-2]=='Do Not Match','Matched_Column'] = np.NAN\n",
    "    \n",
    "    mappedColumns.loc[(mappedColumns.iloc[:,-2]!='Do Not Match') & (mappedColumns.iloc[:,-2]!='Keep Match'),'Matched_Column'] = mappedColumns.iloc[:,-2]\n",
    "    \n",
    "    mappingDF = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    ########## If error on reading, uncomment the following row and... \n",
    "    #mappingDF['df1Name'] = mappedColumns.index\n",
    "    \n",
    "    ########## comment the following row\n",
    "    mappingDF['df2Name'] = mappedColumns['Base_Column']\n",
    "    \n",
    "    mappingDF['df1Name'] = mappedColumns['Matched_Column']\n",
    "    \n",
    "    mappingDF.dropna(inplace=True)\n",
    "    \n",
    "    #print(mappingDF)\n",
    "\n",
    "    origFirstCol = list(firstDF.columns)\n",
    "    orgSecCol= list(secondDF.columns)\n",
    "    \n",
    "    matchColClean = []\n",
    "    \n",
    "    for cols in mappingDF['df1Name']:\n",
    "        if((firstDF[cols].dtype == 'float64') | (firstDF[cols].dtype == 'int')) :\n",
    "            matchColClean.append(cols)\n",
    "        else:\n",
    "            newColName = cols + 'Stripped'\n",
    "            matchColClean.append(newColName)\n",
    "            firstDF[newColName] = firstDF[cols].str.replace('[^\\w\\s]','')\n",
    "            firstDF[newColName] = firstDF[newColName].str.lower()\n",
    "    \n",
    "    mappingDF['df1NamClean'] = pd.Series(matchColClean, index=mappingDF.index)\n",
    "    matchColClean = []\n",
    "    \n",
    "    for cols in mappingDF['df2Name']:\n",
    "        if((secondDF[cols].dtype == 'float64') | (secondDF[cols].dtype == 'int')) :\n",
    "            matchColClean.append(cols)\n",
    "        else:\n",
    "            newColName = cols + 'Stripped'\n",
    "            matchColClean.append(newColName)\n",
    "            secondDF[newColName] = secondDF[cols].str.replace('[^\\w\\s]','')\n",
    "            secondDF[newColName] = secondDF[newColName].str.lower()\n",
    "    \n",
    "    mappingDF['df2NamClean'] = pd.Series(matchColClean, index=mappingDF.index)\n",
    "\n",
    "    firstDF['bigKey'] = \"\"\n",
    "    secondDF['bigKey'] = \"\"\n",
    "    for index, cols in mappingDF.iterrows():\n",
    "        #print(cols)\n",
    "        firstDF['bigKey'] = firstDF['bigKey'].astype(str) + firstDF[cols['df1NamClean']].astype(str)\n",
    "        secondDF['bigKey'] = secondDF['bigKey'].astype(str) + secondDF[cols['df2NamClean']].astype(str)\n",
    "    \n",
    "    #print(firstDF['bigKey'].head())\n",
    "    \n",
    "    output = firstDF.copy()\n",
    "    \n",
    "    writeToStatus(\"Aligning rows\")\n",
    "    \n",
    "    output['matchIndex'] = firstDF.loc[:topXRows,'bigKey'].apply(partial(most_likely, secondDF['bigKey']))\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    results = output.join(secondDF,on='matchIndex',how='right',rsuffix='_ds2',lsuffix='_ds1')\n",
    "    \n",
    "    results['matchCertainty'] = results.loc[:topXRows,'bigKey_ds1'].apply(partial(most_likelyValue, results['bigKey_ds2']))\n",
    "    \n",
    "    dropCol = ['bigKey_ds1','bigKey_ds2']\n",
    "    \n",
    "    for cols in list(results.columns):\n",
    "        if 'Stripped' in cols :\n",
    "            dropCol.append(cols)\n",
    "    \n",
    "    keepCol = list(set(results.columns) - set(list(dropCol)))\n",
    "    \n",
    "    results = results[keepCol]\n",
    "    \n",
    "    writeToStatus(\"Creating output file\")\n",
    "    \n",
    "    for cols in results:\n",
    "        if cols in origFirstCol:\n",
    "            newColName = cols + '_ds1'\n",
    "            results = results.rename(columns={cols:newColName})\n",
    "        if cols in orgSecCol:\n",
    "            newColName = cols + '_ds2'\n",
    "            results = results.rename(columns={cols:newColName})\n",
    "    \n",
    "    results.to_csv('results.csv', encoding='latin-1')\n",
    "    writeToStatus(\"Match N Merge For Rows Completed\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is Rob's code\n",
    "def rowJoiner():\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import difflib as dl\n",
    "    import math\n",
    "    from functools import partial\n",
    "    import os\n",
    "    import string\n",
    "\n",
    "    def str_diff(tarString, testString) :\n",
    "        #returns the ratio difference between 2 strings\n",
    "        seq = dl.SequenceMatcher(None,tarString, str(testString))\n",
    "        ratioNum = seq.ratio()\n",
    "        return ratioNum\n",
    "\n",
    "    def most_likely(testDF, tarString):\n",
    "        #identifies which string in a DF is most like the target string.  Returns the index.\n",
    "        tempDF = pd.DataFrame()\n",
    "        tempDF['distance'] = np.NAN\n",
    "        tempDF['distance'] = testDF.apply(partial(str_diff,tarString))\n",
    "        maxValue = tempDF['distance'].max()\n",
    "        maxIndex = tempDF['distance'].idxmax()\n",
    "            #print('maxValue', maxValue)\n",
    "        int(maxIndex)\n",
    "            #print('maxIndex', maxIndex)\n",
    "        return maxIndex\n",
    "\n",
    "    def most_likelyValue(testDF, tarString):\n",
    "        #identifies which string in a DF is most like the target string.  Returns the ratio value.\n",
    "        tempDF = pd.DataFrame()\n",
    "        tempDF['distance'] = np.NAN\n",
    "        tempDF['distance'] = testDF.apply(partial(str_diff,tarString))\n",
    "        maxValue = tempDF['distance'].max()\n",
    "        maxIndex = tempDF['distance'].idxmax()\n",
    "            #print('maxValue', maxValue)\n",
    "        int(maxIndex)\n",
    "            #print('maxIndex', maxIndex)\n",
    "        return maxValue\n",
    "\n",
    "    writeToStatus(\"Starting row merge\")\n",
    "\n",
    "    #Reading in the two files and which columns are supposed to align.  Incorporates User corrections.\n",
    "    df = pd.read_csv('filesReady.txt', header=None)\n",
    "\n",
    "    file1 = str(df.iloc[0,0])\n",
    "    firstDF = pd.read_csv(file1, encoding='latin-1', index_col=None)\n",
    "\n",
    "    file2 = str(df.iloc[0,1])\n",
    "    secondDF = pd.read_csv(file2, encoding='latin-1', index_col=None)\n",
    "\n",
    "\n",
    "    topXRows = 15\n",
    "\n",
    "    mappedColumns = pd.read_csv(\"columnMapping_user.csv\", encoding='latin-1')\n",
    "\n",
    "        #print('TOTAL DF\\n',mappedColumns)\n",
    "\n",
    "    mappedColumns.loc[mappedColumns.iloc[:,-2]=='Do Not Match','Matched_Column'] = np.NAN\n",
    "\n",
    "    mappedColumns.loc[(mappedColumns.iloc[:,-2]!='Do Not Match') & (mappedColumns.iloc[:,-2]!='Keep Match'),'Matched_Column'] = mappedColumns.iloc[:,-2]\n",
    "\n",
    "    mappingDF = pd.DataFrame()\n",
    "\n",
    "     ########## If error on reading, uncomment the following row and... \n",
    "    #mappingDF['df1Name'] = mappedColumns.index\n",
    "\n",
    "        ########## comment the following row\n",
    "    mappingDF['df2Name'] = mappedColumns['Base_Column']\n",
    "    mappingDF['df1Name'] = mappedColumns['Matched_Column']\n",
    "\n",
    "    mappingDF.dropna(inplace=True)\n",
    "\n",
    "        #print(mappingDF)\n",
    "\n",
    "    origFirstCol = list(firstDF.columns)\n",
    "    orgSecCol= list(secondDF.columns)\n",
    "\n",
    "    matchColClean = []\n",
    "\n",
    "    #Provides basic string cleaning by converting to lowercase and removing punctuation\n",
    "    for cols in mappingDF['df2Name']:\n",
    "        if((firstDF[cols].dtype == 'float64') | (firstDF[cols].dtype == 'int')) :\n",
    "            matchColClean.append(cols)\n",
    "        else:\n",
    "            newColName = cols + 'Stripped'\n",
    "            matchColClean.append(newColName)\n",
    "            firstDF[newColName] = firstDF[cols].str.replace('[^\\w\\s]','')\n",
    "            firstDF[newColName] = firstDF[newColName].str.lower()\n",
    "\n",
    "    mappingDF['df1NamClean'] = pd.Series(matchColClean, index=mappingDF.index)\n",
    "    matchColClean = []\n",
    "\n",
    "    for cols in mappingDF.index.values:\n",
    "        if((secondDF[cols].dtype == 'float64') | (secondDF[cols].dtype == 'int')) :\n",
    "            matchColClean.append(cols)\n",
    "        else:\n",
    "            newColName = cols + 'Stripped'\n",
    "            matchColClean.append(newColName)\n",
    "            secondDF[newColName] = secondDF[cols].str.replace('[^\\w\\s]','')\n",
    "            secondDF[newColName] = secondDF[newColName].str.lower()\n",
    "\n",
    "    mappingDF['df2NamClean'] = pd.Series(matchColClean, index=mappingDF.index)\n",
    "\n",
    "    #Combines all overlapped columns into a single string that is compared against the same combined string\n",
    "    #in the second data set\n",
    "    firstDF['bigKey'] = \"\"\n",
    "    secondDF['bigKey'] = \"\"\n",
    "    for index, cols in mappingDF.iterrows():\n",
    "        #print(cols)\n",
    "        firstDF['bigKey'] = firstDF['bigKey'].astype(str) + firstDF[cols['df1NamClean']].astype(str)\n",
    "        secondDF['bigKey'] = secondDF['bigKey'].astype(str) + secondDF[cols['df2NamClean']].astype(str)\n",
    "\n",
    "    output = firstDF.copy()\n",
    "\n",
    "    writeToStatus(\"Aligning rows\")\n",
    "\n",
    "    output['matchIndex'] = firstDF.loc[:topXRows,'bigKey'].apply(partial(most_likely, secondDF['bigKey']))\n",
    "\n",
    "    #Creates a new DF with the first and second DFs merged based on alignment\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    results = output.join(secondDF,on='matchIndex',how='right',rsuffix='_ds2',lsuffix='_ds1')\n",
    "\n",
    "    results['matchCertainty'] = results.loc[:topXRows,'bigKey_ds1'].apply(partial(most_likelyValue, results['bigKey_ds2']))\n",
    "\n",
    "    #Cleans up Results by dropping temporary columns\n",
    "    dropCol = ['bigKey_ds1','bigKey_ds2']\n",
    "\n",
    "    for cols in list(results.columns):\n",
    "        if 'Stripped' in cols :\n",
    "            dropCol.append(cols)\n",
    "\n",
    "    keepCol = list(set(results.columns) - set(list(dropCol)))\n",
    "\n",
    "    results = results[keepCol]\n",
    "\n",
    "    writeToStatus(\"Creating results output file\")\n",
    "\n",
    "    #Creating output files\n",
    "    for cols in results:\n",
    "        if cols in origFirstCol:\n",
    "            newColName = cols + '_ds1'\n",
    "            results = results.rename(columns={cols:newColName})\n",
    "        if cols in orgSecCol:\n",
    "            newColName = cols + '_ds2'\n",
    "            results = results.rename(columns={cols:newColName})\n",
    "\n",
    "    results.to_csv('results.csv', encoding='latin-1')\n",
    "    writeToStatus(\"Match N Merge For Rows Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rowJoiner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rowJoiner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
